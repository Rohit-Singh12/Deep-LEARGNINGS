# -*- coding: utf-8 -*-
"""Pruning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13hkNUO4sh7SgnTDp_hl7AMDkBmloqwNZ

# Pruning
Pruning involoves removing neurons from the model which don't contibute much to model performance. In this way one can reduce the model size while still maintaining good performance.

### Pruning is done primarliy on two ways-


*   Neuron Pruning: Here the entire nodes are removed from the network resulting in small layers and hence faster inference
*   Weight Pruning: Here some of the weights in the matrices are set to 0 which don't contribute to model performance and the weights are then stored in CSR(Compressed Sparse Row) for efficient resource utilization. It doesn't improve interence time but model size is compressed

## Zero Pruning

It involves removing the weights from the matrices whose weight is close to zero. The idea is that the weight which is close to zero doesn't contribute much to model output. So, set the weights close to 0 to 0 and use CSR to store the sparse matrix efficiently.
"""

!pip install pytorch-lightning

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning.utilities.types import OptimizerLRScheduler
import torchmetrics

"""### Loading MNIST dataset from torcvision.datasets"""

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

trainset = torchvision.datasets.MNIST(
    root='./data',
    train=True,
    download=True,
    transform=transform
)

train_loader = DataLoader(trainset,
                          shuffle=True,
                          batch_size=64)

testset = torchvision.datasets.MNIST(
    root='./data',
    train=False,
    download=True,
    transform=transform
)

test_loader = DataLoader(testset,
                         shuffle=True,
                         batch_size=64)

"""### Create the Network for Training the model"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class CNN(pl.LightningModule):
  def __init__(self):
    super(CNN, self).__init__()
    self.Accuracy = torchmetrics.Accuracy(
        task='multiclass',
        num_classes=10
    )
    self.conv = nn.Conv2d(in_channels=1,
                          out_channels=32,
                          kernel_size=3,
                          stride=1,
                          padding=1,
                          device=device)
    self.pool = nn.MaxPool2d(kernel_size=3,
                             stride=1)  #[batch_size, 32, 26, 26]

    self.fc1 = nn.Linear(32*26*26, 512)
    self.fc2 = nn.Linear(512, 256)
    self.fc3 = nn.Linear(256, 128)
    self.fc4 = nn.Linear(128, 10)

  def forward(self, input):
    X = F.relu(self.conv(input))
    X = self.pool(X)
    X = X.view(X.size(0), -1)
    X = F.relu(self.fc1(X))
    X = F.relu(self.fc2(X))
    X = F.relu(self.fc3(X))
    return self.fc4(X)

  def configure_optimizers(self) -> OptimizerLRScheduler:
    optimizer = torch.optim.Adam(self.parameters(),
                                 lr=0.001)
    schedular = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    return {
        "optimizer": optimizer,
        "lr_scheduler": {
            "scheduler": schedular,
            "interval": "epoch",
            "frequency": 1
        }
    }

  def training_step(self, batch, batch_idx):
    X,y = batch
    logits = self.forward(X)
    loss = F.cross_entropy(logits, y)
    accuracy = self.Accuracy(logits, y)

    self.log_dict({'train_loss': loss, 'accuracy': accuracy},
                  prog_bar=True,
                  on_epoch=True,
                  on_step=False)
    return loss

  def test_step(self, batch, batch_idx):
    X, y = batch
    logits = self.forward(X)
    loss = F.cross_entropy(logits, y)
    accuracy = self.Accuracy(logits, y)

    self.log_dict({'train_loss':loss, 'accuracy':accuracy},
                  prog_bar=True,
                  on_epoch=True,
                  on_step=False)
    return loss

model = CNN()
trainer = pl.Trainer(
    max_epochs=5,
    accelerator='gpu'
)

trainer.fit(model, train_dataloaders=train_loader)

"""#### Define threshold for to zero out all the weights below it"""

trainer.test(model, test_loader)

threshold = 0.06
for name, param in model.named_parameters():
  if 'weight' in name:
    param.data[param.data.abs() < threshold]=0
trainer.test(model, test_loader)

total = 0
zeros = 0
for name, param in model.named_parameters():
    if 'weight' in name:
        total += param.numel()
        zeros += torch.sum(param == 0).item()

print(f"Sparsity: {100 * zeros / total:.2f}%")

"""As can be seen from above the matrix is 93% sparse. We can now use CSR to store these weights efficiently"""

import scipy.sparse as sp

sparse_weights = []
shapes = []

for name, param in model.named_parameters():
    if 'weight' in name:
        np_weight = param.data.cpu().numpy()
        original_shape = np_weight.shape
        shapes.append(original_shape)

        np_weight_2d = np_weight.reshape(np_weight.shape[0], -1)
        sparse_weights.append(sp.csr_matrix(np_weight_2d))

"""## Activation Pruning
It involves removing the weights from the hidden layer which don't contribute to the model performance.
The idea is that if the activation is very close to zero it will not have significant contribution to the output of the model as WX = 0 only

### Algorithm


*   Train the model
*   After training run the training data through the network again without calculating gradients
*   Calculate average activation of each neuron in the hiddent layer
*   Define threshold of pruning
*   Remove the weight matrices below threshold






"""

model = CNN()
trainer = pl.Trainer(
    accelerator='gpu',
    max_epochs=4
)

trainer.fit(model, test_loader)

"""Now apply pruning on the Fully Connected Hidden Layer fc1, fc2 and fc3"""

all_activations = [
    torch.zeros(512), #fc1
    torch.zeros(256), #fc2
    torch.zeros(128) #fc3
]

data_size = 0
for images, _ in train_loader:
  data_size += images.size(0)

  X = F.relu(model.conv(images))
  X = model.pool(X)
  X = X.view(X.size(0), -1)
  activation_fc1 = F.relu(model.fc1(X))
  all_activations[0] += activation_fc1.sum(dim=0)

  activation_fc2 = F.relu(model.fc2(activation_fc1))
  all_activations[1] += activation_fc2.sum(dim=0)

  activation_fc3 = F.relu(model.fc3(activation_fc2))
  all_activations[2] += activation_fc3.sum(dim=0)

  if (images.size(0)*10 == data_size):
    break

  # print([i.shape for i in all_activations])


# Calculating the average activation
for idx, activation in enumerate(all_activations):
  all_activations[idx] = activation/data_size

"""Pruning neurons of fc1"""

threshold = 0.01
new_net = CNN()
new_net.fc1.weight = model.fc1.weight
new_net.fc2.weight = model.fc2.weight
new_net.fc3.weight = model.fc3.weight
new_net.fc4.weight = model.fc4.weight

new_net.fc1.bias = model.fc1.bias
new_net.fc2.bias = model.fc2.bias
new_net.fc3.bias = model.fc3.bias
new_net.fc4.bias = model.fc4.bias

new_net.fc1.weight = nn.Parameter(new_net.fc1.weight[all_activations[0]>=threshold])

new_net.fc2.weight = nn.Parameter(new_net.fc2.weight[:, all_activations[0]>=threshold])
new_net.fc2.weight = nn.Parameter(new_net.fc2.weight[all_activations[1]>=threshold])

new_net.fc3.weight = nn.Parameter(new_net.fc3.weight[:, all_activations[1]>=threshold])
new_net.fc3.weight = nn.Parameter(new_net.fc3.weight[all_activations[2]>=threshold])

new_net.fc4.weight = nn.Parameter(new_net.fc4.weight[:, all_activations[2]>=threshold])


new_net.fc1.bias = nn.Parameter(new_net.fc1.bias[all_activations[0]>=threshold])
new_net.fc2.bias = nn.Parameter(new_net.fc2.bias[all_activations[1]>=threshold])
new_net.fc3.bias = nn.Parameter(new_net.fc3.bias[all_activations[2]>=threshold])

"""Checking the performance"""

trainer.test(model, test_loader)

trainer.test(new_net, test_loader)
