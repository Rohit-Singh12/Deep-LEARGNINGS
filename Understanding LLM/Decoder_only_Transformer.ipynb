{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPArqO0CXerSDEi0sf0MftL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Singh12/Deep-LEARGNINGS/blob/main/Understanding%20LLM/Decoder_only_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoder only Transformer"
      ],
      "metadata": {
        "id": "M6FE01jB_VZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating dataset for training transformers"
      ],
      "metadata": {
        "id": "OZtiZRN5oKZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Vocabulary\n",
        "vocabulary = ['What', 'is', 'Machine', 'Learning', '<EOS>', 'Mathmematics']\n",
        "token_to_id = {word: idx for idx, word in enumerate(vocabulary)}\n",
        "id_to_token = {idx: word for word, idx in token_to_id.items()}\n",
        "\n",
        "# Inputs and labels (Batch size: 2, Sequence length: 6)\n",
        "inputs = torch.tensor([\n",
        "    [token_to_id['What'], token_to_id['is'], token_to_id['Machine'],\n",
        "     token_to_id['Learning'], token_to_id['<EOS>'], token_to_id['Mathmematics']],\n",
        "    [token_to_id['Machine'], token_to_id['Learning'], token_to_id['is'],\n",
        "     token_to_id['What'], token_to_id['<EOS>'], token_to_id['Mathmematics']]\n",
        "])\n",
        "\n",
        "labels = torch.tensor([\n",
        "    [token_to_id['is'], token_to_id['Machine'], token_to_id['Learning'],\n",
        "     token_to_id['<EOS>'], token_to_id['Mathmematics'], token_to_id['<EOS>']],\n",
        "    [token_to_id['Learning'], token_to_id['is'], token_to_id['What'],\n",
        "     token_to_id['<EOS>'], token_to_id['Mathmematics'], token_to_id['<EOS>']]\n",
        "])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8EcbBr8QrjY-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculating Positional Encoding of the inputs"
      ],
      "metadata": {
        "id": "1I6bpGAHoMxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
        "## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model=5, max_len=10):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        embedding_index = torch.arange(0, d_model, 2)\n",
        "        div_term = 1/torch.tensor(10000.0)**(embedding_index / d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term[:pe[:, 1::2].shape[1]])\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.shape[1], :].unsqueeze(0)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eOT6VAFAACmO"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Self Attention"
      ],
      "metadata": {
        "id": "QjaJH_GJoR-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaled Dot-Product Attention with Masking\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, d_model=5):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        #print(\"Dimension of q \", q.shape)\n",
        "        q = self.W_q(q)  # (Batch, Seq, d_model)\n",
        "        k = self.W_k(k)\n",
        "        v = self.W_v(v)\n",
        "        #print(q,k)\n",
        "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_model ** 0.5)  # (Batch, Seq, Seq)\n",
        "\n",
        "        if mask is not None:\n",
        "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        output = torch.matmul(attn_weights, v)  # (Batch, Seq, d_model)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "b35TrY92_-7L"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decoder only Transformer"
      ],
      "metadata": {
        "id": "g2HQCVS_oU6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder-Only Transformer Model\n",
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    def __init__(self, n_tokens, d_model=5, max_len=10):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(n_tokens, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.self_attention = Attention(d_model)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 2, d_model)\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.fc_out = nn.Linear(d_model, n_tokens)\n",
        "        self.loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, token_ids, labels=None):\n",
        "        # print(\"token ids\", token_ids)\n",
        "        x = self.embedding(token_ids)\n",
        "        x = self.pos_encoding(x)\n",
        "        # print(\"shape of x \", x.shape)\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).expand(batch_size, -1, -1).to(x.device)\n",
        "        # print(\"mask \", mask)\n",
        "        attn_output = self.self_attention(x, x, x, mask)\n",
        "        x = self.norm1(x + attn_output)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = self.norm2(x + ffn_output)\n",
        "        # print(\" shape of x after ffn\", x.shape)\n",
        "        logits = self.fc_out(x)  # (Batch, Seq, Vocab_size)\n",
        "        # print(\" shape after output\", logits.shape)\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "            return logits, loss\n",
        "        return logits\n",
        "    def generate(self, start_tokens):\n",
        "        self.eval()  # Set model to evaluation mode\n",
        "        generated_tokens = start_tokens.clone()\n",
        "\n",
        "        while True:\n",
        "            logits = self.forward(generated_tokens)  # Get logits for the current sequence\n",
        "            # print(logits)\n",
        "            next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(1)  # Get most probable next token\n",
        "            generated_tokens = torch.cat((generated_tokens, next_token), dim=1)  # Append next token\n",
        "\n",
        "            if next_token.item() == token_to_id['<EOS>']:  # Stop if EOS token is reached\n",
        "                break\n",
        "\n",
        "        return generated_tokens"
      ],
      "metadata": {
        "id": "yFlLHmKA_899"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "kNSoXI6toXJH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 5\n",
        "max_len = 10\n",
        "n_tokens = len(vocabulary)\n",
        "lr = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "model = DecoderOnlyTransformer(n_tokens, d_model, max_len)\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(inputs, labels)\n",
        "    loss.backward() # Backpropragation step to update weight\n",
        "    optimizer.step() # Update weights\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wQxqyt0l_6TX",
        "outputId": "5c4cb600-74d1-4cfa-c721-ea3ab00e67dc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.9208663702011108\n",
            "Epoch 100, Loss: 0.02862042374908924\n",
            "Epoch 200, Loss: 0.005945244804024696\n",
            "Epoch 300, Loss: 0.002684066304937005\n",
            "Epoch 400, Loss: 0.001552328933030367\n",
            "Epoch 500, Loss: 0.0010218644747510552\n",
            "Epoch 600, Loss: 0.0007280257996171713\n",
            "Epoch 700, Loss: 0.0005466984584927559\n",
            "Epoch 800, Loss: 0.00042633211705833673\n",
            "Epoch 900, Loss: 0.000342007348081097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating output\n"
      ],
      "metadata": {
        "id": "W1wiInevobBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the trained model\n",
        "model = DecoderOnlyTransformer(n_tokens=len(vocabulary))\n",
        "\n",
        "start_seq = torch.tensor([[token_to_id['What'], token_to_id['is'], token_to_id['Machine'], token_to_id['Learning']]])\n",
        "\n",
        "output_tokens = model.generate(start_seq)\n",
        "\n",
        "# Convert token IDs back to words\n",
        "generated_sentence = [id_to_token[token.item()] for token in output_tokens[0]]\n",
        "print(\"Generated Sentence:\", \" \".join(generated_sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2G8TUNCxc_y",
        "outputId": "ec646378-d460-4da8-c9d8-0e358d0fd7af"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence: What is Machine Learning Learning Learning <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3qhuPsNz3xc"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}